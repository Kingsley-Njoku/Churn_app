# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qvNpCxoJhMd0Zzp9wj_G7KcdmV2_v2sJ
"""

# app.py ‚Äì Professional UI version with SHAP
import streamlit as st
import pandas as pd
import numpy as np
import joblib
import os
import shap
import matplotlib.pyplot as plt
from collections import defaultdict

# ======================
# Page config
# ======================
st.set_page_config(
    page_title="Customer Churn Predictor",
    layout="wide",
    page_icon="üìä"
)

# ======================
# Load Saved Model
# ======================
@st.cache_resource
def load_pipeline():
    model_path = "churn_pipeline_v2.joblib"  # adjust if different
    if not os.path.exists(model_path):
        st.error(f"‚ùå Model file not found at: {model_path}")
        st.stop()
    return joblib.load(model_path)

pipeline = load_pipeline()

# ======================
# Helpers
# ======================
def get_preprocessor_from_pipeline(pipe):
    if hasattr(pipe, "named_steps"):
        if "preprocessor" in pipe.named_steps:
            return pipe.named_steps["preprocessor"]
        for name, step in pipe.named_steps.items():
            if hasattr(step, "transform") and not hasattr(step, "predict"):
                return step
    try:
        return pipe.steps[0][1]
    except Exception:
        return None

def get_classifier_from_pipeline(pipe):
    if hasattr(pipe, "named_steps"):
        for candidate in ("clf", "model", "estimator"):
            if candidate in pipe.named_steps:
                return pipe.named_steps[candidate]
        for name, step in pipe.named_steps.items():
            if hasattr(step, "predict_proba") or hasattr(step, "predict"):
                return step
    try:
        return pipe.steps[-1][1]
    except Exception:
        return None

def safe_cast_str_list(seq):
    return [str(x) for x in list(seq)]

def safe_get_feature_names(column_transformer, input_df):
    if column_transformer is None:
        return []
    try:
        if hasattr(column_transformer, "get_feature_names_out"):
            try:
                out = column_transformer.get_feature_names_out(input_df.columns)
            except TypeError:
                out = column_transformer.get_feature_names_out()
            return safe_cast_str_list(out)
    except Exception:
        pass

    names = []
    try:
        for name, transformer, cols in column_transformer.transformers_:
            if name == "remainder":
                continue
            if isinstance(cols, slice):
                resolved_cols = list(input_df.columns)[cols]
            elif isinstance(cols, (list, tuple, np.ndarray, pd.Index)):
                resolved_cols = list(cols)
            else:
                try:
                    resolved_cols = [input_df.columns[cols]]
                except Exception:
                    resolved_cols = [str(cols)]

            ohe = None
            if hasattr(transformer, "named_steps"):
                for key in ("onehot", "ohe"):
                    if key in transformer.named_steps:
                        ohe = transformer.named_steps[key]
                        break
                if ohe is None:
                    last_step = list(transformer.named_steps.values())[-1]
                    if hasattr(last_step, "get_feature_names_out") or hasattr(last_step, "categories_"):
                        ohe = last_step
            else:
                if hasattr(transformer, "get_feature_names_out") or hasattr(transformer, "categories_"):
                    ohe = transformer

            if ohe is not None:
                try:
                    out_names = list(ohe.get_feature_names_out(resolved_cols))
                    names.extend(safe_cast_str_list(out_names))
                except Exception:
                    cats = getattr(ohe, "categories_", None)
                    if cats is None:
                        names.extend(safe_cast_str_list(resolved_cols))
                    else:
                        out_names = []
                        for col, cats_for_col in zip(resolved_cols, cats):
                            for c in cats_for_col:
                                out_names.append(f"{col}_{c}")
                        names.extend(safe_cast_str_list(out_names))
            else:
                names.extend(safe_cast_str_list(resolved_cols))

        for name, transformer, cols in column_transformer.transformers_:
            if name == "remainder":
                if transformer == "passthrough" or transformer == "remainder":
                    if isinstance(cols, slice):
                        cols_res = list(input_df.columns)[cols]
                    elif isinstance(cols, (list, tuple, np.ndarray, pd.Index)):
                        cols_res = list(cols)
                    else:
                        try:
                            cols_res = [input_df.columns[cols]]
                        except Exception:
                            cols_res = [str(cols)]
                    names.extend(safe_cast_str_list(cols_res))
    except Exception:
        return []
    return names

def map_feat_to_original(feat_names, original_cols):
    original_cols_str = [str(c) for c in original_cols]
    feat_to_orig = []
    for f in feat_names:
        f_str = str(f)
        matched = None
        best_len = -1
        for col in original_cols_str:
            if f_str == col:
                matched = col
                best_len = len(col)
                break
        if matched is None:
            for col in original_cols_str:
                prefix = col + "_"
                if f_str.startswith(prefix) and len(col) > best_len:
                    matched = col
                    best_len = len(col)
        feat_to_orig.append(matched if matched is not None else "__OTHER__")
    return feat_to_orig

def grouped_shap_from_featarr(feat_names, shap_arr1d, original_cols):
    feat_to_orig = map_feat_to_original(feat_names, original_cols)
    grouped = defaultdict(float)
    for orig, val in zip(feat_to_orig, shap_arr1d):
        grouped[orig] += float(val)
    return dict(grouped), feat_to_orig

def extract_row_shap_1d(shap_expl):
    vals = getattr(shap_expl, "values", None)
    if vals is None:
        vals = np.array(shap_expl)
    if vals.ndim == 1:
        return vals
    if vals.ndim == 2:
        return vals[0]
    if vals.ndim == 3:
        return vals[0, -1, :]
    raise ValueError(f"Unsupported SHAP shape {vals.shape}")

def extract_sample_shap_2d(shap_expl):
    vals = getattr(shap_expl, "values", None)
    if vals is None:
        try:
            vals = np.array(shap_expl)
        except Exception:
            return None
    if vals.ndim == 2:
        return vals
    if vals.ndim == 3:
        return vals[:, -1, :]
    return None

# ======================
# UI
# ======================
st.markdown(
    """
    <div style="background-color:#1e3d59;padding:15px;border-radius:10px">
    <h2 style="color:white;text-align:center;">üìä Customer Churn Predictor</h2>
    <p style="color:#f5f5f5;text-align:center;">
    Upload your customer dataset and see customers at risk of leaving your company.
    </p>
    </div>
    """,
    unsafe_allow_html=True
)

with st.sidebar:
    st.header("‚öôÔ∏è App Controls")
    uploaded_file = st.file_uploader("Upload CSV/Excel", type=["csv", "xlsx"])
    st.markdown("---")
    st.info("Use this app to analyze churn risk and identify drivers behind predictions.")

if uploaded_file:
    try:
        if uploaded_file.name.endswith(".csv"):
            company_df = pd.read_csv(uploaded_file)
        else:
            company_df = pd.read_excel(uploaded_file)

        st.success(f"‚úÖ File uploaded successfully! Shape: {company_df.shape}")

        # Tabs for workflow
        tab1, tab2, tab3 = st.tabs(["üîç Predictions", "üßê Explain Prediction", "‚¨áÔ∏è Export"])

        # ---------- Predictions ----------
        with tab1:
            try:
                pred_probs = pipeline.predict_proba(company_df)[:, 1]
            except Exception as e:
                st.error("Model could not predict on uploaded data. Ensure columns match training schema.")
                st.exception(e)
                raise e

            company_df["churn_probability"] = pred_probs
            company_df["churn_risk"] = pd.cut(
                company_df["churn_probability"],
                bins=[0, 0.5, 0.7, 1.0],
                labels=["Low", "Medium", "High"]
            )

            st.dataframe(company_df.head(20), use_container_width=True, height=400)

        # ---------- Explanation ----------
        with tab2:
            id_candidates = ["customer_id", "CustomerID", "name", "Name", "ID"]
            id_col = next((c for c in id_candidates if c in company_df.columns), None)
            if id_col:
                selected_customer = st.selectbox(
                    "Select a customer",
                    company_df[id_col].astype(str).tolist()
                )
                row_data = company_df[company_df[id_col].astype(str) == selected_customer].iloc[[0]]
                selected_customer_display = row_data[id_col].values[0]
            else:
                row_index = st.number_input(
                    "Enter row number (0 = first customer)",
                    min_value=0,
                    max_value=max(0, len(company_df)-1),
                    value=0
                )
                row_data = company_df.iloc[[row_index]]
                selected_customer_display = f"row_{row_index}"

            st.write("### Selected Customer Data")
            st.dataframe(row_data, use_container_width=True)

            with st.spinner("Computing SHAP explanations..."):
                preprocessor = get_preprocessor_from_pipeline(pipeline)
                classifier = get_classifier_from_pipeline(pipeline)

                if preprocessor is None or classifier is None:
                    st.warning("Preprocessor or classifier not found in pipeline ‚Äî cannot compute SHAP.")
                else:
                    X_all_proc = preprocessor.transform(company_df)
                    X_row_proc = preprocessor.transform(row_data)

                    feat_names = safe_get_feature_names(preprocessor, company_df)
                    if not feat_names or len(feat_names) != X_all_proc.shape[1]:
                        feat_names = [f"f{i}" for i in range(X_all_proc.shape[1])]

                    try:
                        explainer = shap.Explainer(classifier, X_all_proc, feature_names=feat_names)
                    except Exception:
                        explainer = shap.Explainer(classifier)

                    shap_row = explainer(X_row_proc)
                    row_vals = extract_row_shap_1d(shap_row)

                    grouped_row, _ = grouped_shap_from_featarr(feat_names, row_vals, list(company_df.columns))
                    grouped_series = pd.Series(grouped_row).reindex(
                        pd.Series(grouped_row).abs().sort_values(ascending=False).index
                    )
                    display_series = grouped_series.head(10)

                    st.write("#### Top Feature Contributions")
                    fig, ax = plt.subplots(figsize=(10, max(3, 0.35*len(display_series))))
                    vals = display_series.values[::-1]
                    labels = list(display_series.index[::-1])
                    colors = ['#ff3b6f' if v > 0 else '#2b9cff' for v in vals]

                    y_pos = np.arange(len(vals))
                    bars = ax.barh(y_pos, vals, color=colors)

                    ax.set_yticks(y_pos)
                    ax.set_yticklabels(labels)
                    ax.set_xlabel("SHAP value (impact on churn)")
                    ax.grid(axis='x', linestyle='--', alpha=0.4)
                    plt.tight_layout()

                    for i, bar in enumerate(bars):
                        v = bar.get_width()
                        iy = bar.get_y() + bar.get_height()/2
                        offset = (ax.get_xlim()[1] - ax.get_xlim()[0]) * 0.01
                        xpos = v + offset if v >= 0 else v - offset
                        ha = "left" if v >= 0 else "right"
                        ax.text(xpos, iy, f"{v:.3f}", va="center", ha=ha, fontsize=9, color="black")

                    st.pyplot(fig, use_container_width=True)
                    plt.clf()

                    # plain-English
                    st.write("### üìù Plain-English Explanation")
                    top3 = sorted(grouped_row.items(), key=lambda x: abs(x[1]), reverse=True)[:3]
                    reasons = []
                    for name, impact in top3:
                        direction = "increased" if float(impact) > 0 else "decreased"
                        reasons.append(f"**{name}** {direction} churn risk (impact {impact:.3f})")

                    pred_prob = float(row_data["churn_probability"].values[0])
                    risk_label = row_data["churn_risk"].values[0]

                    st.markdown(
                        f"Customer **{selected_customer_display}** has a predicted churn probability of "
                        f"**{pred_prob:.2f}** (risk: **{risk_label}**)."
                    )
                    if reasons:
                        st.markdown("**Top reasons:** " + "; ".join(reasons))

        # ---------- Export ----------
        with tab3:
            st.write("Download the full predictions with risk levels.")
            csv = company_df.to_csv(index=False).encode("utf-8")
            st.download_button(
                "‚¨áÔ∏è Download CSV",
                data=csv,
                file_name="churn_predictions.csv",
                mime="text/csv"
            )

            excel_path = "churn_predictions.xlsx"
            company_df.to_excel(excel_path, index=False)
            with open(excel_path, "rb") as f:
                st.download_button(
                    "‚¨áÔ∏è Download Excel",
                    data=f,
                    file_name="churn_predictions.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

    except Exception as e:
        st.error(f"‚ö†Ô∏è Error processing file: {e}")
        import traceback
        traceback.print_exc()

